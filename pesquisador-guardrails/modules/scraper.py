from crewai import Agent, Task
from crewai_tools import ScrapeWebsiteTool
from typing import List, Optional
import os
from utils.guardrails import filter_valid_links
from pydantic import Field

class SafeScrapeWebsiteTool(ScrapeWebsiteTool):
    disallowed_domains: List[str] = Field(default_factory=list)

    def __init__(self, disallowed_domains: Optional[List[str]] = None):
        super().__init__()
        self.disallowed_domains = disallowed_domains or []

    def _execute(self, url: str) -> str:
        if isinstance(url, dict):
            url = url.get("website_url", "")
            
        if any(domain in url for domain in self.disallowed_domains):
            print(f"üö´ Tentativa de acessar dom√≠nio proibido bloqueada: {url}")
            return None
            
        return super()._execute(url)

def create_scraper_agent(tema, model_name):
    scrap_tool = SafeScrapeWebsiteTool(disallowed_domains=["www.youtube.com", "www.jusbrasil.com", "gov.br"])
    return Agent(
        role="Preparar um texto com as informa√ß√µes extra√≠das de cinco sites mencionados em um noticias_filtradas.md. Sites ileg√≠veis devem ser ignorados.",
        goal="Fazer um jur√≠dico, perfeitamente escrito e bem estruturado com base nas informa√ß√µes informa√ß√µes encontradas nos sites mencionados pelo agente pesquisador sobre o tema, extraindo as principais informa√ß√µes para fins jur√≠dicos, como leis, jurisprud√™ncia e doutrinas mencionados.",
        backstory="""Voc√™ √© um profundo conhecedor do direito brasileiro. Sua principal qualidade √© conseguir compilar informa√ß√µes de um site para sempre fornecer o m√°ximo de informa√ß√µes para os outros agentes. 
        O texto deve ser o mais completo poss√≠vel e preparado para fornecer informa√ß√µes relevantes e corretas sobre o tema aos demais agentes. Ignore sites ileg√≠veis.
        """,
        verbose=True,
        tools=[scrap_tool],
        llm=model_name,
        max_inter=5,
    )

# modules/scraper.py

def create_scrap_task(tema, output_dir, agent, filtered_links, disallowed_domains):
    """
    Cria uma tarefa para o agente Scraper com links j√° filtrados.

    Args:
        tema (str): Tema da pesquisa.
        output_dir (str): Diret√≥rio de sa√≠da.
        agent (Agent): Agente respons√°vel pela tarefa.
        filtered_links (list): Lista de links filtrados.
        disallowed_domains (list): Lista de dom√≠nios proibidos.

    Returns:
        Task: Objeto de tarefa configurado.
    """
    try:
        # Filtrar links proibidos
        filtered_links = filter_valid_links(filtered_links, disallowed_domains)

        # Verificar se h√° links v√°lidos ap√≥s o filtro
        if not filtered_links:
            raise ValueError(f"‚ö†Ô∏è A lista de links fornecida n√£o cont√©m links v√°lidos ap√≥s o filtro.")

        # Criar a tarefa usando os links validados
        return Task(
            description=(
                f"Usar os links filtrados para compilar informa√ß√µes jur√≠dicas "
                f"relevantes sobre o tema {tema}."
            ),
            expected_output=f"Texto completo sobre {tema} com links v√°lidos extra√≠dos dos sites mencionados.",
            tools=agent.tools,
            inputs={"links": filtered_links},  # Passar links como entrada
            output_file=os.path.join(output_dir, "scrap.md"),
            agent=agent,
        )
    except ValueError as e:
        print(f"‚ö†Ô∏è {e}")
        return None